# Tokenization ðŸª™

Tokenization is the first step in data preparation, specialized for converting input text into individual tokens ids.